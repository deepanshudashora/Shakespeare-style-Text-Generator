{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkQlgtFzHDkD",
        "outputId": "548958fc-9788-4a53-9baa-c2c4404e74af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCRgtd7JGx-8",
        "outputId": "e0a75343-5a0e-4240-948d-de1600b46adb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "didn't crash yet!\n",
            "> Hello, I'm a language model, not a program.\n",
            "\n",
            "So this morning I started studying for the interview in the lab. This was not\n",
            "> Hello, I'm a language model, and one of the main things that bothers me when they create languages is how easy it becomes to create something that\n",
            "> Hello, I'm a language model, and I wrote it off on the grounds that a language model would make me more fluent. But I'm not\n",
            "> Hello, I'm a language model, I really like languages. I like languages because like, they're good. And the way we talk about languages\n",
            "> Hello, I'm a language model, a language model I'm using for data modelling. All I did was test the results and then I wrote some\n",
            "Model Parameters (name and shape):\n",
            "Total Parameters: 162.35M\n",
            "step 0: loss 0.2199, lr 0.000000\n",
            "New best loss: 0.2199 - Saving model...\n",
            "step 100: loss 8.6011, lr 0.000005\n",
            "step 200: loss 8.3040, lr 0.000010\n",
            "step 300: loss 7.2572, lr 0.000015\n",
            "step 400: loss 6.3143, lr 0.000020\n",
            "step 500: loss 5.6861, lr 0.000025\n",
            "step 600: loss 5.2858, lr 0.000030\n",
            "step 700: loss 5.0145, lr 0.000035\n",
            "step 800: loss 4.8127, lr 0.000040\n",
            "step 900: loss 4.6757, lr 0.000045\n",
            "step 1000: loss 4.5286, lr 0.000050\n",
            "step 1100: loss 4.4477, lr 0.000055\n",
            "step 1200: loss 4.3364, lr 0.000060\n",
            "step 1300: loss 4.2295, lr 0.000065\n",
            "step 1400: loss 4.1212, lr 0.000070\n",
            "step 1500: loss 4.0399, lr 0.000075\n",
            "step 1600: loss 3.9334, lr 0.000080\n",
            "step 1700: loss 3.8221, lr 0.000085\n",
            "step 1800: loss 3.7355, lr 0.000090\n",
            "step 1900: loss 3.6491, lr 0.000095\n",
            "step 2000: loss 3.5456, lr 0.000100\n",
            "step 2100: loss 3.4397, lr 0.000100\n",
            "step 2200: loss 3.3379, lr 0.000100\n",
            "step 2300: loss 3.2295, lr 0.000100\n",
            "step 2400: loss 3.1348, lr 0.000100\n",
            "step 2500: loss 2.9991, lr 0.000100\n",
            "step 2600: loss 2.8419, lr 0.000100\n",
            "step 2700: loss 2.7209, lr 0.000100\n",
            "step 2800: loss 2.5794, lr 0.000100\n",
            "step 2900: loss 2.4247, lr 0.000100\n",
            "step 3000: loss 2.2492, lr 0.000100\n",
            "step 3100: loss 2.0794, lr 0.000099\n",
            "step 3200: loss 1.9038, lr 0.000099\n",
            "step 3300: loss 1.7628, lr 0.000099\n",
            "step 3400: loss 1.5719, lr 0.000099\n",
            "step 3500: loss 1.4208, lr 0.000099\n",
            "step 3600: loss 1.2720, lr 0.000099\n",
            "step 3700: loss 1.1486, lr 0.000099\n",
            "step 3800: loss 1.0083, lr 0.000099\n",
            "step 3900: loss 0.8787, lr 0.000098\n",
            "step 4000: loss 0.7605, lr 0.000098\n",
            "step 4100: loss 0.6547, lr 0.000098\n",
            "step 4200: loss 0.5852, lr 0.000098\n",
            "step 4300: loss 0.5152, lr 0.000098\n",
            "step 4400: loss 0.4507, lr 0.000098\n",
            "step 4500: loss 0.4150, lr 0.000097\n",
            "step 4600: loss 0.3771, lr 0.000097\n",
            "step 4700: loss 0.3479, lr 0.000097\n",
            "step 4800: loss 0.3223, lr 0.000097\n",
            "step 4900: loss 0.3023, lr 0.000097\n",
            "step 5000: loss 0.2877, lr 0.000096\n",
            "step 5100: loss 0.2756, lr 0.000096\n",
            "step 5200: loss 0.2644, lr 0.000096\n",
            "step 5300: loss 0.2561, lr 0.000096\n",
            "step 5400: loss 0.2455, lr 0.000095\n",
            "step 5500: loss 0.2358, lr 0.000095\n",
            "step 5600: loss 0.2287, lr 0.000095\n",
            "step 5700: loss 0.2229, lr 0.000094\n",
            "step 5800: loss 0.2170, lr 0.000094\n",
            "New best loss: 0.2170 - Saving model...\n",
            "step 5900: loss 0.2124, lr 0.000094\n",
            "New best loss: 0.2124 - Saving model...\n",
            "step 6000: loss 0.2092, lr 0.000093\n",
            "New best loss: 0.2092 - Saving model...\n",
            "step 6100: loss 0.2020, lr 0.000093\n",
            "New best loss: 0.2020 - Saving model...\n",
            "step 6200: loss 0.1983, lr 0.000093\n",
            "New best loss: 0.1983 - Saving model...\n",
            "step 6300: loss 0.1927, lr 0.000092\n",
            "New best loss: 0.1927 - Saving model...\n",
            "step 6400: loss 0.1905, lr 0.000092\n",
            "New best loss: 0.1905 - Saving model...\n",
            "step 6500: loss 0.1877, lr 0.000092\n",
            "New best loss: 0.1877 - Saving model...\n",
            "step 6600: loss 0.1853, lr 0.000091\n",
            "New best loss: 0.1853 - Saving model...\n",
            "step 6700: loss 0.1800, lr 0.000091\n",
            "New best loss: 0.1800 - Saving model...\n",
            "step 6800: loss 0.1781, lr 0.000091\n",
            "New best loss: 0.1781 - Saving model...\n",
            "step 6900: loss 0.1754, lr 0.000090\n",
            "New best loss: 0.1754 - Saving model...\n",
            "step 7000: loss 0.1723, lr 0.000090\n",
            "New best loss: 0.1723 - Saving model...\n",
            "step 7100: loss 0.1694, lr 0.000090\n",
            "New best loss: 0.1694 - Saving model...\n",
            "step 7200: loss 0.1670, lr 0.000089\n",
            "New best loss: 0.1670 - Saving model...\n",
            "step 7300: loss 0.1644, lr 0.000089\n",
            "New best loss: 0.1644 - Saving model...\n",
            "step 7400: loss 0.1625, lr 0.000088\n",
            "New best loss: 0.1625 - Saving model...\n",
            "step 7500: loss 0.1614, lr 0.000088\n",
            "New best loss: 0.1614 - Saving model...\n",
            "step 7600: loss 0.1593, lr 0.000087\n",
            "New best loss: 0.1593 - Saving model...\n",
            "step 7700: loss 0.1575, lr 0.000087\n",
            "New best loss: 0.1575 - Saving model...\n",
            "step 7800: loss 0.1564, lr 0.000087\n",
            "New best loss: 0.1564 - Saving model...\n",
            "step 7900: loss 0.1524, lr 0.000086\n",
            "New best loss: 0.1524 - Saving model...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "from train_get2_1 import GPT, GPTConfig  # Import the model architecture\n",
        "\n",
        "# Enhanced hyperparameters\n",
        "BATCH_SIZE = 24  # Adjusted for memory constraints\n",
        "BLOCK_SIZE = 128  # Keep smaller block size for efficiency\n",
        "MAX_ITERS = 25000  # Increased iterations to compensate for smaller batch size\n",
        "EVAL_INTERVAL = 100\n",
        "LEARNING_RATE = 1e-4  # Reduced learning rate for smaller batch size\n",
        "WARMUP_ITERS = 2000  # Extended warmup period\n",
        "MIN_LR = 1e-5\n",
        "WEIGHT_DECAY = 0.01  # Reduced weight decay for smaller batch\n",
        "GRAD_CLIP = 0.5  # Reduced gradient clipping\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "n_embd = 768\n",
        "n_head = 12\n",
        "n_layer = 12\n",
        "dropout = 0.1  # Reduced dropout for smaller batch\n",
        "\n",
        "# Load and preprocess the Shakespeare text\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Initialize tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "vocab_size = enc.n_vocab\n",
        "\n",
        "# Encode the entire text\n",
        "data = torch.tensor(enc.encode(text), dtype=torch.long)\n",
        "\n",
        "def get_batch():\n",
        "    # Add data augmentation: randomly offset sequences\n",
        "    ix = torch.randint(len(data) - BLOCK_SIZE - 1, (BATCH_SIZE,))\n",
        "    offset = torch.randint(0, 2, (BATCH_SIZE,))  # Random offset of 0 or 1\n",
        "    ix = ix + offset\n",
        "    x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "    x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "    return x, y\n",
        "\n",
        "def get_lr(iter):\n",
        "    # Implement learning rate scheduling\n",
        "    if iter < WARMUP_ITERS:\n",
        "        return LEARNING_RATE * iter / WARMUP_ITERS\n",
        "    decay_ratio = (iter - WARMUP_ITERS) / (MAX_ITERS - WARMUP_ITERS)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # cosine decay\n",
        "    return MIN_LR + coeff * (LEARNING_RATE - MIN_LR)\n",
        "\n",
        "# Model init\n",
        "model_config = GPTConfig(\n",
        "    block_size=BLOCK_SIZE,\n",
        "    vocab_size=vocab_size,\n",
        "    n_layer=n_layer,\n",
        "    n_head=n_head,\n",
        "    n_embd=n_embd,\n",
        ")\n",
        "model = GPT(model_config)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Print model parameters\n",
        "print(\"Model Parameters (name and shape):\")\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total Parameters: {total_params/1e6:.2f}M\")\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    betas=(0.9, 0.999)  # More conservative beta values\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "best_loss = float('inf')\n",
        "running_loss = 0.0\n",
        "beta = 0.98  # Increased smoothing for loss calculation\n",
        "for iter in range(MAX_ITERS):\n",
        "    # Learning rate scheduling\n",
        "    lr = get_lr(iter)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Sample a batch of data\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # Calculate running loss\n",
        "    running_loss = beta * running_loss + (1 - beta) * loss.item()\n",
        "\n",
        "    # Print progress and save model if loss improves\n",
        "    if iter % EVAL_INTERVAL == 0:\n",
        "        print(f\"step {iter}: loss {running_loss:.4f}, lr {lr:.6f}\")\n",
        "        if running_loss < best_loss:\n",
        "            best_loss = running_loss\n",
        "            print(f\"New best loss: {best_loss:.4f} - Saving model...\")\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'iter': iter,\n",
        "                'best_loss': best_loss,\n",
        "            }, 'shakespeare_model.pt')\n",
        "\n",
        "    # Backward pass with gradient clipping\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "    optimizer.step()\n",
        "\n",
        "# Generate sample text after training\n",
        "model.eval()\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=DEVICE)\n",
        "print(enc.decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7K01PpbHAxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}